week 1:
embedding layers
mean layers

N-grams: need a lot of space and RAM
Recurrent Neural Networds
RNNs Basic Structure: Wh, Wx, W
An RNN world have the same number of parameters for word sequences of different lengths

Application of RNNs
One to One: given some scores of a championship, you can predict the winner. 
One to Many: given an image, you can predict what the caption is going to be.
Many to One: given a tweet, you can predict the sentiment of that tweet
Many to Many: given an english sentence, you can translate it to its German equivalent

Feed Forward, Math in Simple RNNs
Hidden state is used to pass info to next state
y_hat use current hidden state to calculate


Cross Entropy Loss for RNNs
tf.scan(fn, elems, initializer,...)

Gated Recurrent Units (GRUs)
relevance gates: 
- input: previous hidden state, x
- activation: sigmoid
- use to calculate hidden state candidate, multiple with previous, concat with x, go through Wh, tanh => candidate hidden state
- 0 mean not relevance just use x
- 1 mean relevance just both x and previous hidden state
update gates:
- input: previous hidden state, x
- activation: sigmoid
- use to calculate hidden state
- 0 mean don't update, use previous hidden state, don't use hidden state candidate
- 1 mean update all, don't use previous hidden state, use hidden state candidate

Bi-directional RNNs
Deep RNNs

Evaluate with perplexity

week 2:
advantages of RNNs:
- capture dependencies within a short range
- less RAM than other n-gram models
disadvantages of RNNs"
- struggle with longer term dependencies
- very prone to vanishing or exploding grdients

solutions to vanishing gradient problems:
- identity RNN with ReLU activation
- gradient clipping
- skip connections

LSTMs
memorable solutions: learns when to remember and when to forget

basic anatomy:
-a cell state
-a hidden state
-multiple gates

gates allow gradients to avoid vanishing and exploding

discard anything irrelevant
add important new information
produce output

forget gate:
- input: hidden previous state, x
- activation sigmoid
- used to update cell state, decide what info should forget or not
- 0 means forget all
- 1 means don't forget

input gate:
- input: previous hidden state, x
- activation tanh, can try other
- used to update cell state, decide what info should add to cell state
- 0 means don't add to input gate
- 1 means add to input gate

candidate cell state:
- input: hidden previous state, x
- activation tanh, can try other
- used to update cell state
- multiple with input gate, then add previous cell state to calculate current cell state

output gate:
- input: hidden previous state, x
- activation sigmoid
- used to calculate current hidden state
- multiple with cell state transform

cell state transform:
- input: current cell state
- activation tanh, some LSTMs use linear activation
- used to calculate current hidden state
- multiple with output gate

y_hat, output:
- input current hidden state

applications of LSTMs
- next-character prediction
- chatbots
- music composition
- image captioning
- speech recognition

Named Entity Recognition (NER)
Application of NER systems
- search engine efficiency
- recommendation engines
- customer service
- automatic trading

1. convert words and entity classes into same-length numberical arrays
token padding:
- for LSTMs, all sequences need to be the same size
- set sequence length to a certain number
- use the <PAD> token to fill empty spaces
2. train in batches for faster processing
3. feed it into an LSTM Units
4. run the output through a dense layer
5. Predict using a log softmax over K classes

To compare the accuracy, just follow the following steps:
Pass test set through the model 
Get arg max across the prediction array
Mask padded tokens
Compare with the true labels. 

week 3:
Siamese Network
applications of siamese networks:
- handwritten checks
- quetions duplicates 
- queries

sister-networks, sub-networks share identical paramenters
1. embedding
2. LSTM
3. Vectors
4. Cosine Similarity, can use other similarity

anchor, positive, negative
diff = -cos(A, P) + cos(A, N)

triplets
alpha margin, hyparamenters
loss(A, P, N) = max(diff + alpha, 0)
random triplet selection: nothing to learn
easy negative triplet: cos(A, N) << cos(A, P)
semi-hard negative triplet: cos(A, N) ~ cos(A, P), cos(A, N) < cos(A, P)
hard negative triplet: cos(A, P) < cos(A, N)

batch 1 vs batch 2

mean_neg: speeds up training
closest_neg: helps penalize the cost more 
cost1 = max(-cos(A,P) + mean_neg + alpha, 0)
cost2 = max(-cos(A,P) + closest_neg + alpha, 0)
full_cost = cost1 + cost2

one shot learning: measures similarity between 2 classes
classification: classfies input as 1 of K classes

testing:
1. Convert two inputs into an array of numbers
2. Feed it into your model
3. Compare 𝒗1,𝒗2 using cosine similarity
4. Test against a threshold