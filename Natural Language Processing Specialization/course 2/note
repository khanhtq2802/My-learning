week 1:
identify a misspelled word
find strings n edit distance away: insert, delete, switch, replace
filter candidates
calulate word probabilities

minimum edit distance algorithm

week 2:
part of speed tagging
you can use POS tagging for: 
- Identifying named entities
- Speech recognition
- Coreference Resolution

Markov Chains, Hidden Markov Models
transition probabilities
emission probabilities

viterbi algorithm
initialization
forward pass
backward pass

week 3:
N-Grams
unigrams, bigrams, trigrams
problem: corpus almost never contains exact sentence we're interested in or even its longer subsequences

approximation of sequence probability: bigrams, trigrams,..., N-gram
markov assumption: only last N words matter

starting and ending sentences: 
bigrams: <s> </s>
trigram: <s><s> </s>
N-grams model: add N - 1 start tokens <s>
</s> has 2 motivation

perplexity, log perplexity

criteria to create the vocabulary:
min word frequency f
Max |V|, include words by frequency
Use <UNK> sparingly (Why?)
Perplexity -  only compare LMs with the same V

smoothing:
add-one smoothing(Laplacian smoothing)
add-k smoothing
advanced methods: Kneser-Ney smoothing, Good-Turing smoothing
backoff, stupid backoff, interpolation

week 4:
word embeddings
self-supervised

word embeddings methods:
classical methods: word2vec, Continuous bag-of-words (CBOW), Continuous skip-gram / Skip-gram with negative sampling (SGNS), Global Vectors (GloVe), fastText
deep learning, contextual embeddings: BERT, ELMo, GPT-2

Continuous bag-of-words
center word
context words, sliding window